PataNet Vision — Setup & Operations Guide

Reprodutibilidade do ambiente, datasets, API, demo visual e pipeline de avaliação/calibração para identificação de espécie/raça e busca por similaridade (casos de animais perdidos).

0) Sumário

Visão geral

Pré-requisitos

Clonar e instalar

Estrutura de pastas (ignoradas no Git)

Datasets: manual e automático

Subir a API (FastAPI/Uvicorn)

Demo visual (Streamlit)

Avaliação em lote (eval)

Grid Search / Calibração

Como o score é calculado

Automação (bootstrap)

Boas práticas de performance

Troubleshooting

API Reference rápida

1) Visão geral

PataNet Vision identifica espécie/raça e retorna imagens semelhantes para auxiliar em reconhecimento e busca de animais perdidos.

Componentes principais:

API (FastAPI/Uvicorn) em app/: indexa embeddings (FAISS), atende /search, agrega espécie/raça e explica o resultado.

Demo (Streamlit) em demo/: UI simples para upload → Top-K, percentuais e atributos.

Indexação (FAISS + opcional PCA/normalização) persistida em index/.

Curadoria/Eval/Grid (scripts/prepare_eval_and_grid.py): monta eval/, avalia e calibra pesos.

2) Pré-requisitos

Python 3.10 ou 3.11

Git (Git LFS opcional para modelos maiores)

PowerShell (Windows) ou Bash (Linux/macOS)

FAISS: no Windows prefira faiss-cpu via pip

3) Clonar e instalar
git clone https://github.com/Willen-Leolatto/patanet-vision.git patanet-vision
cd patanet-vision

Windows (PowerShell)
python -m venv .venv
.\.venv\Scripts\Activate
pip install --upgrade pip
pip install -r requirements.txt
# Se necessário no Windows:
pip install faiss-cpu==1.8.0

Linux/macOS
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

4) Estrutura local (não versionada)

Estas pastas são ignoradas pelo Git e você deve criá-las localmente:

index/    # índices FAISS/PCA e artefatos de busca
outputs/  # CSVs/figuras de avaliação e grid search
calib/    # resultados de calibração (pesos/relatórios)
data/     # datasets brutos (Stanford Dogs, Oxford Pets etc.)
eval/     # subconjunto balanceado para avaliação


Criar rapidamente:

Linux/macOS

mkdir -p index outputs calib data eval


Windows (PowerShell)

mkdir index, outputs, calib, data, eval

5) Datasets

Você pode preparar os dados manualmente (controle total) ou via curadoria automática para gerar eval/.

5.1 Manual (controle total)

Estrutura recomendada:

data/stanford_dogs/Images/<classe>/*.jpg
data/oxford_pets/images/*.jpg
data/oxford_pets/annotations/{trimaps,xmls}/*


Links oficiais dos datasets (procure “Stanford Dogs Dataset” e “Oxford-IIIT Pet Dataset”).
Descompacte mantendo a estrutura de pastas acima.

5.2 Curadoria automática (gera eval/)

Seleciona N imagens por classe para avaliação.

python scripts/prepare_eval_and_grid.py curate --per-class 20 --dst eval


Ajuste --per-class (ex.: 15, 20, 30…) conforme tempo e espaço em disco.

6) Subir a API

Em um terminal:

uvicorn app.main:app --host 127.0.0.1 --port 8000 --workers 4


Carrega/gera índices em index/ (a primeira vez pode demorar mais).

Ajuste --workers conforme CPU.

7) Demo visual (site)

Em outro terminal:

streamlit run demo/app.py


A demo consome http://127.0.0.1:8000/search.

Mostra a consulta, Top-K (com percentuais e atributos), rótulos previstos e imagens similares.

Se a demo acusar erro de conexão, verifique se a API está rodando.

8) Avaliação em lote

Avalia o conjunto eval/, gera métricas, confusões e CSVs.

Windows (PowerShell)
python scripts/prepare_eval_and_grid.py eval ^
  --api http://127.0.0.1:8000 ^
  --root eval ^
  --workers 4 ^
  --timeout 60 ^
  --sleep-between 0.00 ^
  --out-csv outputs/eval_results.csv ^
  --resume

Linux/macOS
python scripts/prepare_eval_and_grid.py eval \
  --api http://127.0.0.1:8000 \
  --root eval \
  --workers 4 \
  --timeout 60 \
  --sleep-between 0.00 \
  --out-csv outputs/eval_results.csv \
  --resume


Saídas esperadas em outputs/:

eval_results.csv

per_class_summary.csv

confusion_matrix.csv e/ou confusion_matrix_top25.csv

top_confusions.csv, hard_classes.csv

figuras (heatmap/curvas) e metrics.json

Use --resume para continuar de onde parou sem perder progresso.

9) Grid Search / Calibração

Explora pesos/hiperparâmetros do ranqueamento (k-reciprocal + cabeça + cor) e objetivos.

Exemplo leve (rodadas rápidas)
Windows (PowerShell)
python scripts/prepare_eval_and_grid.py grid ^
  --api http://127.0.0.1:8000 ^
  --root eval ^
  --max-images 600 ^
  --workers 4 ^
  --timeout 60 ^
  --sleep-between 0.00 ^
  --objective mixed ^
  --sweep "w_krec=[0.4,0.6,0.8];w_head=[0.1,0.2];w_color=[0.05,0.1];krec_k1=[12,20];krec_k2=[4,6];krec_lambda=[0.3,0.5]" ^
  --out outputs/grid_round1.csv ^
  --resume

Linux/macOS
python scripts/prepare_eval_and_grid.py grid \
  --api http://127.0.0.1:8000 \
  --root eval \
  --max-images 600 \
  --workers 4 \
  --timeout 60 \
  --sleep-between 0.00 \
  --objective mixed \
  --sweep "w_krec=[0.4,0.6,0.8];w_head=[0.1,0.2];w_color=[0.05,0.1];krec_k1=[12,20];krec_k2=[4,6];krec_lambda=[0.3,0.5]" \
  --out outputs/grid_round1.csv \
  --resume


--objective: breed | retrieval | mixed

Ao final, selecione a melhor combinação (coluna de objetivo) e fixe na API (variáveis no app/main.py ou .env conforme sua versão).

10) Como o score é calculado (intuição)

O ranqueamento final combina três sinais:

score_final = w_krec * score_kreciprocal
            + w_head * score_head
            + w_color * score_color


K-reciprocal re-ranking (k1/k2/λ): utiliza “vizinhos de vizinhos” para reduzir intrusos e refinar a ordem.

Head detector/embedding (“cabeça”): foca em rosto/orelhas/olhos, crucial para separar raças parecidas.

Color (histogramas/HSV): ajuda em empates e em diferenças de pelagem.

Espécie (dog/cat): classificador leve que filtra confusões grossas.

Raça: consenso do Top-K (maioria ponderada) + auxiliar de classificação (quando disponível).
A demo exibe percentuais normalizados por Top-K e explica brevemente os fatores.

11) Automação (bootstrap)
PowerShell — scripts/bootstrap.ps1
param(
  [int]$PerClass=20,
  [string]$ApiHost="127.0.0.1",
  [int]$ApiPort=8000
)

$dirs = @("index","outputs","calib","data","eval")
$dirs | % { if (!(Test-Path $_)) { New-Item -ItemType Directory -Name $_ | Out-Null } }

python -m venv .venv
.\.venv\Scripts\Activate
pip install --upgrade pip
pip install -r requirements.txt

# Curadoria rápida p/ avaliação
python scripts/prepare_eval_and_grid.py curate --per-class $PerClass --dst eval

Write-Host ""
Write-Host "API:  uvicorn app.main:app --host $ApiHost --port $ApiPort --workers 4"
Write-Host "Demo: streamlit run demo/app.py"

Bash — scripts/bootstrap.sh
#!/usr/bin/env bash
set -e

mkdir -p index outputs calib data eval
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# Curadoria rápida p/ avaliação
python scripts/prepare_eval_and_grid.py curate --per-class 20 --dst eval

echo
echo "API:  uvicorn app.main:app --host 127.0.0.1 --port 8000 --workers 4"
echo "Demo: streamlit run demo/app.py"

12) Dicas de performance

Primeira indexação em index/ é a mais cara; depois, o carregamento é rápido. Guarde essa pasta.

Use --max-images menor no grid (ex.: 300–600) para rodadas rápidas; refine depois.

Sempre use --resume no eval/grid para não refazer o que já foi processado.

Mantenha index/ e eval/ em SSD.

Rode API e eval/grid em terminais separados.

Para classes “difíceis”, aumente exemplos no eval/ ou ajuste pesos w_*/krec_*.

13) Troubleshooting rápido
Sintoma	Causa comum	Ação
Connection refused ao chamar /search	API não está rodando	uvicorn app.main:app --host 127.0.0.1 --port 8000 --workers 4
ModuleNotFoundError: requests	venv não ativada ou deps faltando	Ative venv e pip install -r requirements.txt
FAISS no Windows	Build nativo difícil	pip install faiss-cpu==1.8.0
Aviso CRLF/LF do Git	Final de linha divergente	Opcional: git config core.autocrlf true
Streamlit avisando use_column_width depreciado	API de layout atualizada	Use use_container_width=True nas imagens
14) API Reference rápida
Versão / Saúde
GET /version
→ {
  "model": "...",
  "use_fast": true,
  "device": "cpu",
  "ntotal": 26111,
  "dim": 256,
  "gallery_pca": true,
  "head_index": true,
  "head_lookup": 26111
}

Busca
POST /search
Content-Type: multipart/form-data
fields:
  - file: <imagem>
  - k: int (default 5)
  - use_head: 0/1
  - use_color: 0/1
  - w_krec, w_head, w_color (floats)
  - krec_k1, krec_k2 (ints), krec_lambda (float)


Resposta (resumo):

{
  "ok": true,
  "latency_ms": 1000.2,
  "quality_ok": true,
  "k": 5,
  "breed_top3": [
    {"label":"golden_retriever","prob":0.5870},
    {"label":"irish_setter","prob":0.0372},
    {"label":"cocker_spaniel","prob":0.0322}
  ],
  "attributes": {
    "coat": "tom predominante amarelado/dourado, brilho médio",
    "chest_white": false,
    "ear": null
  },
  "explanation": "Raça provável: golden retriever (0.59); ...",
  "topk": [
    {
      "score": 0.4732,
      "score_pct": 47.3,
      "score_krecip": 0.285,
      "score_head": 0.7756,
      "score_color": 0.7219,
      "weights": {"krec":0.6,"head":0.25,"color":0.15},
      "species_pred": "dog",
      "species_conf": 0.261,
      "crop": "outputs/crops/....jpg",
      "src":  "data/stanford_dogs/Images/...jpg",
      "cls": 16
    }
  ],
  "flags": {"use_head": true, "use_color": true}
}


Exemplo curl:

curl -X POST "http://127.0.0.1:8000/search" \
  -F "file=@teste.jpg" \
  -F "k=8" \
  -F "use_head=1" -F "use_color=1" \
  -F "w_krec=0.6" -F "w_head=0.25" -F "w_color=0.15" \
  -F "krec_k1=20" -F "krec_k2=6" -F "krec_lambda=0.3"
